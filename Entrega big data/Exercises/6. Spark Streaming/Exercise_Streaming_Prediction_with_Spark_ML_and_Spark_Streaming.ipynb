{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3a86f8df",
      "metadata": {
        "id": "3a86f8df"
      },
      "source": [
        "# Hands-on Exercise_Streaming Prediction with Spark ML and Spark Streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d06b6c8",
      "metadata": {
        "id": "7d06b6c8"
      },
      "source": [
        "In this notebook we are going to load a pipeline that has a set of pre-processing phases and a classification model to predict a patient's probability of suffering a heart attack. The prediction will be made on streaming data obtained from the csv of heart.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6318a507",
      "metadata": {
        "id": "6318a507"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f81f759",
      "metadata": {
        "id": "1f81f759"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import OneHotEncoder\n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import OneHotEncoder\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56cecab8",
      "metadata": {
        "id": "56cecab8"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "## Start a Spark session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afe3c31c",
      "metadata": {
        "id": "afe3c31c"
      },
      "outputs": [],
      "source": [
        "## Load and view the csv of Exercises\\data\\heart.csv with the name of heart\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8216cc4",
      "metadata": {
        "id": "e8216cc4"
      },
      "outputs": [],
      "source": [
        "schema = StructType( \\\n",
        "                     [StructField(\"age\", LongType(),True), \\\n",
        "                      StructField(\"sex\", LongType(), True), \\\n",
        "                      StructField(\"cp\", LongType(), True), \\\n",
        "                      StructField('trestbps', LongType(), True), \\\n",
        "                      StructField(\"chol\", LongType(), True), \\\n",
        "                      StructField(\"fbs\", LongType(), True), \\\n",
        "                      StructField(\"restecg\", LongType(), True), \\\n",
        "                      StructField(\"thalach\", LongType(), True),\\\n",
        "                      StructField(\"exang\", LongType(), True), \\\n",
        "                      StructField(\"oldpeak\", DoubleType(), True), \\\n",
        "                      StructField(\"slope\", LongType(),True), \\\n",
        "                      StructField(\"ca\", LongType(), True), \\\n",
        "                      StructField(\"thal\", LongType(), True), \\\n",
        "                      StructField(\"target\", LongType(), True), \\\n",
        "                        ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28d8812f",
      "metadata": {
        "id": "28d8812f",
        "outputId": "8a9974c6-bdbd-4855-d96d-47ea3c6f81d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- sex: integer (nullable = true)\n",
            " |-- cp: integer (nullable = true)\n",
            " |-- trtbps: integer (nullable = true)\n",
            " |-- chol: integer (nullable = true)\n",
            " |-- fbs: integer (nullable = true)\n",
            " |-- restecg: integer (nullable = true)\n",
            " |-- thalachh: integer (nullable = true)\n",
            " |-- exng: integer (nullable = true)\n",
            " |-- oldpeak: double (nullable = true)\n",
            " |-- slp: integer (nullable = true)\n",
            " |-- caa: integer (nullable = true)\n",
            " |-- thall: integer (nullable = true)\n",
            " |-- label: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.types import StructType,StructField,LongType, StringType,DoubleType,TimestampType\n",
        "\n",
        "\n",
        "df = heart.withColumnRenamed(\"target\",\"label\")\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b2c3b4b",
      "metadata": {
        "id": "1b2c3b4b"
      },
      "outputs": [],
      "source": [
        "testDF, trainDF = df.randomSplit([0.3, 0.7])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1a8d0d8",
      "metadata": {
        "id": "b1a8d0d8"
      },
      "source": [
        "### Pipeline loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "088afddf",
      "metadata": {
        "id": "088afddf",
        "outputId": "ad5d35cc-ba8d-42db-f602-7dda80f71722"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PipelineModel_f8d4b32a5360"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.ml import PipelineModel\n",
        "\n",
        "\n",
        "pModel = PipelineModel.load(\"\\pipelines\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f7be455",
      "metadata": {
        "id": "6f7be455"
      },
      "outputs": [],
      "source": [
        "## Check that the above pipeline works correctly. To do so, make a prediction on\n",
        "## trainDF data and show the prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2077daa",
      "metadata": {
        "scrolled": true,
        "id": "c2077daa"
      },
      "outputs": [],
      "source": [
        "testData = testDF.repartition(10)\n",
        "\n",
        "testData.write.format(\"CSV\").option(\"header\",False).save(\"/heart_streaming/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "984bbf3e",
      "metadata": {
        "id": "984bbf3e"
      },
      "source": [
        "## Creating Streaming Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f8489bd",
      "metadata": {
        "id": "7f8489bd"
      },
      "outputs": [],
      "source": [
        "## Use the csv files stored in data/heart_streaming to simulate a streaming data process.\n",
        "## To do so, use the function spark.readStream \n",
        "## In the configuration: set a one file be imported per execution\n",
        "## rename the variable \"output\" to \"label\"\n",
        "## Call this process with the name sourceStream\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55c9160b",
      "metadata": {
        "id": "55c9160b"
      },
      "outputs": [],
      "source": [
        "## Use the \"pModel\" pipeline to make predictions using the streaming data from \"sourceStream\"\n",
        "## From the prediction select the variables: label, probability, prediction. \n",
        "## Call this process with the name \"prediction1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c6113aa",
      "metadata": {
        "scrolled": true,
        "id": "4c6113aa",
        "outputId": "1ae385a2-af6b-4437-8742-677049f1a01f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[label: bigint, probability: vector, prediction: double]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(prediction1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d50112be",
      "metadata": {
        "id": "d50112be"
      },
      "source": [
        "#### Displaying the predictions in console"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "836cdda8",
      "metadata": {
        "id": "836cdda8"
      },
      "outputs": [],
      "source": [
        "## Get the predictions using prediction1.writeStream. in the options\n",
        "## config set: \"format\" equal to \"console\"\n",
        "## in .trigger equals (once=True),\n",
        "## and allow the process to wait for completion with .awaitTermination()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86d638e3",
      "metadata": {
        "id": "86d638e3"
      },
      "source": [
        "#### Keeping the predictions in Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb2672a1",
      "metadata": {
        "id": "eb2672a1"
      },
      "outputs": [],
      "source": [
        "## Get the predictions using prediction1.writeStream.\n",
        "## In configuration: results should be saved in memory \n",
        "## .outputMode should be \"append\"\n",
        "## the name of the query \"queryName\" is \"prediction4\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34147a93",
      "metadata": {
        "id": "34147a93",
        "outputId": "4b6fd15a-bf7c-44d2-9a72-7c04693967f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+----------+\n",
            "|label|         probability|prediction|\n",
            "+-----+--------------------+----------+\n",
            "|    1|[0.04086978924170...|       1.0|\n",
            "|    0|[0.98184892212735...|       0.0|\n",
            "|    1|[0.00474279761632...|       1.0|\n",
            "|    1|[0.35775366097494...|       1.0|\n",
            "|    1|[0.05755909903937...|       1.0|\n",
            "|    0|[0.95305536703752...|       0.0|\n",
            "|    0|[0.94079962605713...|       0.0|\n",
            "|    0|[0.13017480179914...|       1.0|\n",
            "|    0|[0.99807916786174...|       0.0|\n",
            "|    1|[0.15541832735450...|       1.0|\n",
            "+-----+--------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+-----+--------------------+----------+\n",
            "|label|         probability|prediction|\n",
            "+-----+--------------------+----------+\n",
            "|    1|[0.04086978924170...|       1.0|\n",
            "|    0|[0.98184892212735...|       0.0|\n",
            "|    1|[0.00474279761632...|       1.0|\n",
            "|    1|[0.35775366097494...|       1.0|\n",
            "|    1|[0.05755909903937...|       1.0|\n",
            "|    0|[0.95305536703752...|       0.0|\n",
            "|    0|[0.94079962605713...|       0.0|\n",
            "|    0|[0.13017480179914...|       1.0|\n",
            "|    0|[0.99807916786174...|       0.0|\n",
            "|    1|[0.15541832735450...|       1.0|\n",
            "+-----+--------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DataFrame[label: bigint, probability: vector, prediction: double]"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for x in range(2):\n",
        "    df = spark.sql(\n",
        "        \"SELECT * FROM prediction4\")\n",
        "    df.show(10)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9940d9fa",
      "metadata": {
        "id": "9940d9fa"
      },
      "outputs": [],
      "source": [
        "## Validate that the streaming process is active and then show the status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3391605d",
      "metadata": {
        "id": "3391605d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}